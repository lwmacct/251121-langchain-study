"""
Streamlit + LangGraph å·¥å…·è°ƒç”¨æ¼”ç¤º
====================================
å±•ç¤ºå¦‚ä½•åœ¨ Streamlit ä¸­é›†æˆ LangGraph å’Œå·¥å…·è°ƒç”¨åŠŸèƒ½
"""

import os
import operator
from typing import Annotated, Sequence, TypedDict

import streamlit as st
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode

# å¯¼å…¥å·¥å…·
from m_tools import get_current_time, calculator

# ===== é…ç½® API =====
api_key = os.getenv("OPENROUTER_API_KEY")
if not api_key:
    raise RuntimeError("é…ç½®é”™è¯¯:æœªæ‰¾åˆ°ç¯å¢ƒå˜é‡ OPENROUTER_API_KEY")

# åˆ›å»º LLM å®ä¾‹
llm = ChatOpenAI(
    model="anthropic/claude-sonnet-4.5",
    base_url=os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1"),
    api_key=api_key,
    temperature=0.7,
    streaming=True,
)

# ===== é…ç½®å·¥å…· =====
tools = [get_current_time, calculator]
tool_node = ToolNode(tools)
llm_with_tools = llm.bind_tools(tools)


# ===== LangGraph çŠ¶æ€å®šä¹‰ =====
class State(TypedDict):
    """LangGraph çŠ¶æ€"""

    messages: Annotated[Sequence[AnyMessage], operator.add]


def call_model(state: State) -> State:
    """è°ƒç”¨ LLMï¼ˆå¸¦å·¥å…·ï¼‰"""
    response = llm_with_tools.invoke(state["messages"])
    return {"messages": [response]}


def should_continue(state: State):
    """åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    if last_message.tool_calls:
        return "tools"
    return END


# ===== æ„å»º LangGraph =====
graph = StateGraph(State)
graph.add_node("model", call_model)
graph.add_node("tools", tool_node)
graph.add_edge(START, "model")
graph.add_conditional_edges("model", should_continue, {"tools": "tools", END: END})
graph.add_edge("tools", "model")
app = graph.compile()


# ===== Streamlit UI =====
st.set_page_config(page_title="LangGraph + Streamlit å·¥å…·è°ƒç”¨æ¼”ç¤º", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– LangGraph + Streamlit å·¥å…·è°ƒç”¨æ¼”ç¤º")

# ä¾§è¾¹æ ï¼šè¯´æ˜å’Œé¢„è®¾é—®é¢˜
with st.sidebar:
    st.header("âœ¨ åŠŸèƒ½è¯´æ˜")
    st.markdown(
        """
    è¿™æ˜¯ä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

    **ğŸ”§ å¯ç”¨å·¥å…·**
    - `get_current_time` - è·å–å½“å‰æ—¶é—´
    - `calculator` - æ‰§è¡Œæ•°å­¦è®¡ç®—

    **ğŸ‘ï¸ å¯è§†åŒ–**
    - å®æ—¶æ˜¾ç¤ºå·¥å…·è°ƒç”¨è¿‡ç¨‹
    - å±•ç¤ºå·¥å…·å‚æ•°å’Œè¿”å›ç»“æœ
    """
    )

    st.divider()
    st.header("ğŸ¬ é¢„è®¾é—®é¢˜")

    # é¢„è®¾é—®é¢˜æŒ‰é’®
    if st.button("â° æŸ¥è¯¢å½“å‰æ—¶é—´", use_container_width=True):
        st.session_state["preset_question"] = "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿè¯·å‘Šè¯‰æˆ‘å½“å‰çš„æ—¶é—´ã€‚"

    if st.button("ğŸ”¢ æ•°å­¦è®¡ç®—", use_container_width=True):
        st.session_state["preset_question"] = "è¯·å¸®æˆ‘è®¡ç®— 42 * 7 ç­‰äºå¤šå°‘ï¼Ÿ"

    if st.button("ğŸ”§ å¤šå·¥å…·ç»„åˆ", use_container_width=True):
        st.session_state["preset_question"] = "ç°åœ¨å‡ ç‚¹äº†ï¼Ÿå¦å¤–å¸®æˆ‘ç®—ä¸€ä¸‹ 100 é™¤ä»¥ 4 ç­‰äºå¤šå°‘ã€‚"

    if st.button("ğŸ¤– AGI é¢„æµ‹", use_container_width=True):
        st.session_state["preset_question"] = "ä½ è§‰å¾—äººå·¥æ™ºèƒ½ AGI åœ¨å¤šå°‘å¹´åå®ç°ï¼Œé‚£æ—¶æ˜¯å‡ å‡ å¹´ï¼Ÿ"

    if st.button("ğŸ’¬ æ™®é€šå¯¹è¯", use_container_width=True):
        st.session_state["preset_question"] = "ä½ å¥½ï¼è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±å’Œä½ çš„èƒ½åŠ›ã€‚"

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# å¤„ç†é¢„è®¾é—®é¢˜
if "preset_question" in st.session_state:
    prompt = st.session_state.preset_question
    del st.session_state.preset_question
else:
    prompt = st.chat_input("è¯´ç‚¹ä»€ä¹ˆå§...")

# å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt:
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # è°ƒç”¨ LangGraph å¤„ç†
    with st.chat_message("assistant"):
        # åˆ›å»ºå ä½ç¬¦ç”¨äºæ˜¾ç¤ºè¿‡ç¨‹
        status_placeholder = st.empty()
        tool_info_placeholder = st.container()
        response_placeholder = st.empty()

        # æ„å»ºæ¶ˆæ¯
        human_message = HumanMessage(content=prompt)

        # æ”¶é›†æ‰€æœ‰çŠ¶æ€
        final_response = None
        tool_calls_info = []

        try:
            # ä½¿ç”¨çŠ¶æ€æ æ˜¾ç¤ºå¤„ç†çŠ¶æ€
            with status_placeholder.status("ğŸ¤” æ€è€ƒä¸­...", expanded=True) as status:
                # æµå¼å¤„ç†
                for event in app.stream({"messages": [human_message]}, stream_mode="values"):
                    messages = event.get("messages", [])
                    if not messages:
                        continue

                    last_message = messages[-1]

                    # æ£€æµ‹ AI æ¶ˆæ¯ä¸”æœ‰å·¥å…·è°ƒç”¨
                    if isinstance(last_message, AIMessage) and last_message.tool_calls:
                        status.update(label="ğŸ”§ è°ƒç”¨å·¥å…·ä¸­...", state="running")

                        for tool_call in last_message.tool_calls:
                            tool_name = tool_call.get("name", "unknown")
                            tool_args = tool_call.get("args", {})

                            tool_calls_info.append(
                                {
                                    "type": "call",
                                    "name": tool_name,
                                    "args": tool_args,
                                }
                            )

                    # æ£€æµ‹å·¥å…·æ¶ˆæ¯ï¼ˆå·¥å…·è¿”å›ç»“æœï¼‰
                    elif isinstance(last_message, ToolMessage):
                        tool_name = getattr(last_message, "name", "unknown")
                        tool_result = last_message.content

                        tool_calls_info.append(
                            {
                                "type": "result",
                                "name": tool_name,
                                "result": tool_result,
                            }
                        )

                    # æ›´æ–°æœ€ç»ˆå“åº”
                    final_response = last_message

                status.update(label="âœ… å¤„ç†å®Œæˆ", state="complete")

            # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ä¿¡æ¯
            if tool_calls_info:
                with tool_info_placeholder.expander("ğŸ”§ å·¥å…·è°ƒç”¨è¯¦æƒ…", expanded=True):
                    for i, info in enumerate(tool_calls_info):
                        if info["type"] == "call":
                            st.markdown(f"**ğŸ”§ è°ƒç”¨å·¥å…·:** `{info['name']}`")
                            st.code(f"å‚æ•°: {info['args']}", language="python")
                        elif info["type"] == "result":
                            st.markdown(f"**âœ… å·¥å…·ç»“æœ:** `{info['name']}`")
                            st.success(info["result"])

                        if i < len(tool_calls_info) - 1:
                            st.divider()

            # æ˜¾ç¤ºæœ€ç»ˆå“åº”
            if final_response and hasattr(final_response, "content"):
                response_content = final_response.content
                response_placeholder.markdown(response_content)
                st.session_state.messages.append({"role": "assistant", "content": response_content})
            else:
                error_msg = "å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰æ”¶åˆ°å“åº”"
                response_placeholder.warning(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": error_msg})

        except Exception as e:
            status_placeholder.empty()
            error_msg = f"âŒ é”™è¯¯: {type(e).__name__}: {str(e)}"
            response_placeholder.error(error_msg)
            st.session_state.messages.append({"role": "assistant", "content": error_msg})


# ===== é¡µè„šä¿¡æ¯ =====
st.divider()
st.caption("ğŸ’¡ æç¤º: å°è¯•é—®æˆ‘æ—¶é—´ã€è®©æˆ‘åšæ•°å­¦è®¡ç®—ï¼Œæˆ–è€…ç›´æ¥èŠå¤©ï¼")
