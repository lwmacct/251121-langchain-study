"""
App6: å¼‚æ­¥èŠå¤©åº”ç”¨ï¼Œåº•éƒ¨å›ºå®šè¾“å…¥åŒº + ä¸Šæ–¹æ»šåŠ¨è¾“å‡º

ç‰¹æ€§ï¼š
- âœ¨ ç­‰å¾… LLM å“åº”æ—¶å¯ç«‹å³è¾“å…¥ä¸‹ä¸€æ¡æ¶ˆæ¯
- ğŸ“œ ä¸Šæ–¹ä½¿ç”¨æ ‡å‡†ç»ˆç«¯è¾“å‡ºï¼ˆRich printï¼‰ï¼Œè‡ªç„¶æ»šåŠ¨
- ğŸ“ åº•éƒ¨å›ºå®šè¾“å…¥æ¡†ï¼ˆprompt_toolkit Applicationï¼‰
- âš¡ åå°ä»»åŠ¡å®Œæˆååœ¨ä¸‹æ¬¡è¾“å…¥å‰æ˜¾ç¤ºï¼ˆé¿å…è§†è§‰å†²çªï¼‰
- ğŸ“Š å®æ—¶æ˜¾ç¤ºå¾…å¤„ç†ä»»åŠ¡æ•°é‡

ç•Œé¢è®¾è®¡ï¼š
  ç”¨æˆ·: ä½ å¥½                          â†‘ ç»ˆç«¯æ»šåŠ¨åŒºåŸŸ
  åŠ©æ‰‹: [LLM:chat] ä½ å¥½ï¼...          â†‘ Rich print è¾“å‡º
  ç”¨æˆ·: ç°åœ¨å‡ ç‚¹                      â†‘ è‡ªç„¶å‘ä¸Šæ»šåŠ¨
  åŠ©æ‰‹: [get_current_time] 23:00      â†‘
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† åˆ†å‰²çº¿
  1> è¾“å…¥å†…å®¹...                      â† å›ºå®šåœ¨åº•éƒ¨
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† åˆ†å‰²çº¿
  â³ 2 ä¸ªå¤„ç†ä¸­ | Ctrl+J æ¢è¡Œ | Enter å‘é€

å·¥ä½œæµç¨‹ï¼š
  1. å¾ªç¯å¼€å§‹ï¼šæ˜¾ç¤ºæ‰€æœ‰å·²å®Œæˆä»»åŠ¡çš„è¾“å‡ºï¼ˆRich printï¼‰
  2. å¯åŠ¨ Application è·å–è¾“å…¥ï¼ˆå›ºå®šåœ¨åº•éƒ¨ï¼‰
  3. ç”¨æˆ·è¾“å…¥å®Œæˆï¼ŒApplication é€€å‡º
  4. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼Œå¯åŠ¨åå°ä»»åŠ¡
  5. å›åˆ°æ­¥éª¤ 1ï¼ˆåå°ä»»åŠ¡åœ¨è¿è¡Œï¼Œä½†ä¸é˜»å¡è¾“å…¥å¾ªç¯ï¼‰

æŠ€æœ¯è¦ç‚¹ï¼š
- è¾“å‡ºæ€»æ˜¯åœ¨ Application ä¸è¿è¡Œæ—¶è¿›è¡Œ
- Application ä»…ç”¨äºè·å–è¾“å…¥ï¼Œå›ºå®šåœ¨åº•éƒ¨
- è¾“å‡ºä½¿ç”¨ Rich printï¼Œè‡ªç„¶å‘ä¸Šæ»šåŠ¨
- å®Œç¾å®ç°"ä¸Šæ–¹æ»šåŠ¨ + åº•éƒ¨å›ºå®š"çš„æ•ˆæœ
"""

import argparse
import asyncio
import sys

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

# æ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—è¿è¡Œä¸¤ç§æ–¹å¼
if __name__ == "__main__" and __package__ is None:
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    from apps.app6.config import OPENROUTER_API_KEY, OPENROUTER_BASE_URL, MODEL, TEMPERATURE, MAX_TOKENS
    from apps.app6.tools import get_current_time, end_chat
    from apps.app6.router import route_intent_async, render_history
    from apps.app6.ui import AsyncChatUI, print_user, print_assistant
else:
    from .config import OPENROUTER_API_KEY, OPENROUTER_BASE_URL, MODEL, TEMPERATURE, MAX_TOKENS
    from .tools import get_current_time, end_chat
    from .router import route_intent_async, render_history
    from .ui import AsyncChatUI, print_user, print_assistant


def parse_args():
    parser = argparse.ArgumentParser(description="App6: å¼‚æ­¥èŠå¤©åº”ç”¨")
    parser.add_argument("-i", "--interactive", action="store_true",
                        help="ç®¡é“ç»“æŸåè¿›å…¥äº¤äº’æ¨¡å¼")
    return parser.parse_args()


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    args = parse_args()

    llm = ChatOpenAI(
        api_key=OPENROUTER_API_KEY,
        base_url=OPENROUTER_BASE_URL,
        model=MODEL,
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
    )

    # å¤„ç†ç®¡é“è¾“å…¥
    is_piped = not sys.stdin.isatty()

    # åˆå§‹åŒ–å†å²
    history: list = []

    if is_piped:
        piped_lines = [line.strip() for line in sys.stdin.read().splitlines() if line.strip()]

        # ç®¡é“æ¨¡å¼ï¼šåŒæ­¥å¤„ç†æ¯æ¡æ¶ˆæ¯
        for line in piped_lines:
            print_user(line)
            history.append(HumanMessage(content=line))

            action = await route_intent_async(llm, line, history)

            if action == "time":
                result = get_current_time.invoke({})
                reply = f"å½“å‰æ—¶é—´ï¼š{result}"
                history.append(AIMessage(content=reply))
                print_assistant(reply, tool_name="get_current_time")

            elif action == "end":
                result = end_chat.invoke({"reason": line})
                history.append(AIMessage(content=result))
                print_assistant(result, tool_name="end_chat")
                return

            elif action == "summary":
                sys_msg = SystemMessage(content="ç”¨ 2-3 å¥è¯ç®€æ´æ€»ç»“è¿™æ®µå¯¹è¯çš„ä¸»è¦å†…å®¹ã€‚")
                human_msg = HumanMessage(content=f"å¯¹è¯å†…å®¹ï¼š\n{render_history(history)}")
                try:
                    resp = await llm.ainvoke([sys_msg, human_msg])
                    reply = resp.content
                except Exception as exc:
                    reply = f"æ€»ç»“å¤±è´¥ï¼š{exc}"
                history.append(AIMessage(content=reply))
                print_assistant(reply, tool_name="LLM:summary")

            else:  # chat
                sys_msg = SystemMessage(
                    content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ä¸­æ–‡åŠ©æ‰‹ã€‚æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœç”¨æˆ·é—®æ—¶é—´æ®µï¼ˆå¦‚ä¸Šåˆ/ä¸‹åˆï¼‰ï¼Œè¯·åŸºäºä¹‹å‰è·å–çš„æ—¶é—´ä¿¡æ¯å›ç­”ã€‚"
                )
                try:
                    resp = await llm.ainvoke([sys_msg] + history)
                    reply = resp.content
                except Exception as exc:
                    reply = f"å›ç­”å¤±è´¥ï¼š{exc}"
                history.append(AIMessage(content=reply))
                print_assistant(reply, tool_name="LLM:chat")

        if not args.interactive:
            return

    # äº¤äº’æ¨¡å¼ï¼šåº•éƒ¨å›ºå®šè¾“å…¥åŒº + ä¸Šæ–¹æ»šåŠ¨è¾“å‡º
    ui = AsyncChatUI()
    pending_tasks = {}
    task_counter = 0
    pending_outputs = []  # å¾…è¾“å‡ºçš„æ¶ˆæ¯é˜Ÿåˆ—
    should_exit = False

    async def process_message(msg: str):
        """åå°å¤„ç†æ¶ˆæ¯ï¼Œå®Œæˆæ—¶æ·»åŠ åˆ°è¾“å‡ºé˜Ÿåˆ—"""
        try:
            action = await route_intent_async(llm, msg, history)

            if action == "time":
                result = get_current_time.invoke({})
                reply = f"å½“å‰æ—¶é—´ï¼š{result}"
                history.append(AIMessage(content=reply))
                pending_outputs.append(("assistant", reply, "get_current_time"))

            elif action == "end":
                result = end_chat.invoke({"reason": msg})
                history.append(AIMessage(content=result))
                pending_outputs.append(("assistant", result, "end_chat"))
                nonlocal should_exit
                should_exit = True

            elif action == "summary":
                sys_msg = SystemMessage(content="ç”¨ 2-3 å¥è¯ç®€æ´æ€»ç»“è¿™æ®µå¯¹è¯çš„ä¸»è¦å†…å®¹ã€‚")
                human_msg = HumanMessage(content=f"å¯¹è¯å†…å®¹ï¼š\n{render_history(history)}")
                try:
                    resp = await llm.ainvoke([sys_msg, human_msg])
                    reply = resp.content
                except Exception as exc:
                    reply = f"æ€»ç»“å¤±è´¥ï¼š{exc}"
                history.append(AIMessage(content=reply))
                pending_outputs.append(("assistant", reply, "LLM:summary"))

            else:  # chat
                sys_msg = SystemMessage(
                    content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ä¸­æ–‡åŠ©æ‰‹ã€‚æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                )
                try:
                    resp = await llm.ainvoke([sys_msg] + history)
                    reply = resp.content
                except Exception as exc:
                    reply = f"å›ç­”å¤±è´¥ï¼š{exc}"
                history.append(AIMessage(content=reply))
                pending_outputs.append(("assistant", reply, "LLM:chat"))

        except Exception as e:
            pending_outputs.append(("assistant", f"é”™è¯¯: {e}", "Error"))

    while not should_exit:
        # 1. ç­‰å¾…å¹¶æ˜¾ç¤ºæ‰€æœ‰å·²å®Œæˆçš„ä»»åŠ¡è¾“å‡º
        done_ids = [tid for tid, task in pending_tasks.items() if task.done()]
        for tid in done_ids:
            task = pending_tasks.pop(tid)
            try:
                await task
            except Exception:
                pass

        # 2. æ˜¾ç¤ºæ‰€æœ‰å¾…è¾“å‡ºçš„æ¶ˆæ¯ï¼ˆåœ¨ Application è¿è¡Œå‰ï¼‰
        while pending_outputs:
            output_type, content, tool_name = pending_outputs.pop(0)
            if output_type == "assistant":
                print_assistant(content, tool_name)

        # 3. æ›´æ–°å¾…å¤„ç†è®¡æ•°
        ui.pending_count = len(pending_tasks)

        # 4. è·å–ç”¨æˆ·è¾“å…¥ï¼ˆApplication è¿è¡Œï¼‰
        user_input = await ui.get_input_async()
        if user_input is None:
            break

        # 5. æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼ˆApplication é€€å‡ºåï¼‰
        print_user(user_input)
        history.append(HumanMessage(content=user_input))

        # 6. å¯åŠ¨åå°ä»»åŠ¡
        task_counter += 1
        task = asyncio.create_task(process_message(user_input))
        pending_tasks[task_counter] = task

    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    for task in list(pending_tasks.values()):
        if not task.done():
            await task

    # æ˜¾ç¤ºå‰©ä½™è¾“å‡º
    while pending_outputs:
        output_type, content, tool_name = pending_outputs.pop(0)
        if output_type == "assistant":
            print_assistant(content, tool_name)


def main():
    """å…¥å£å‡½æ•°"""
    asyncio.run(async_main())


if __name__ == "__main__":
    main()
