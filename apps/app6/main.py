"""
App6: å¼‚æ­¥èŠå¤©åº”ç”¨ï¼ŒåŸºäº app5 ç•Œé¢ + å¼‚æ­¥èƒ½åŠ›

ç‰¹æ€§ï¼š
- âœ¨ ç­‰å¾… LLM å“åº”æ—¶å¯ç«‹å³è¾“å…¥ä¸‹ä¸€æ¡æ¶ˆæ¯
- ğŸ¨ ä½¿ç”¨ app5 çš„ç•Œé¢æ•ˆæœï¼ˆåˆ†å‰²çº¿ã€è¡Œå·ã€æç¤ºæ ï¼‰
- ğŸš€ å¼‚æ­¥å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œå“åº”åœ¨ä¸‹æ¬¡è¾“å…¥å‰æ‰¹é‡æ˜¾ç¤º
- ğŸ“Š å®æ—¶æ˜¾ç¤ºå¾…å¤„ç†ä»»åŠ¡æ•°é‡ï¼ˆ"â³ N ä¸ªå¤„ç†ä¸­"ï¼‰

ç•Œé¢è®¾è®¡ï¼ˆä¸ app5 ç›¸åŒï¼‰ï¼š
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1> ç¬¬ä¸€è¡Œæ–‡æœ¬
  2> ç¬¬äºŒè¡Œæ–‡æœ¬
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â³ 2 ä¸ªå¤„ç†ä¸­ | Ctrl+J æ¢è¡Œ | Enter å‘é€ | è¿æŒ‰ä¸¤æ¬¡ Ctrl+C é€€å‡º

å·¥ä½œæµç¨‹ï¼š
  1. ç”¨æˆ·è¾“å…¥æ¶ˆæ¯ï¼ŒæŒ‰ Enter æäº¤
  2. ç«‹å³æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ï¼Œå¯åŠ¨åå° LLM ä»»åŠ¡
  3. ç«‹å³è¿”å›è¾“å…¥ç•Œé¢ï¼Œå¯ä»¥ç»§ç»­è¾“å…¥ä¸‹ä¸€æ¡
  4. åå°ä»»åŠ¡å®Œæˆæ—¶ï¼Œå“åº”æš‚å­˜åˆ° pending_outputs
  5. ä¸‹æ¬¡è·å–è¾“å…¥å‰ï¼Œè‡ªåŠ¨æ˜¾ç¤ºæ‰€æœ‰å·²å®Œæˆçš„å“åº”

æŠ€æœ¯è¦ç‚¹ï¼š
- ä½¿ç”¨ Application.run_async() æ”¯æŒå¼‚æ­¥
- è¾“å‡ºæš‚å­˜æœºåˆ¶é¿å… Application è¿è¡Œæ—¶è¾“å‡ºå¯¼è‡´ç•Œé¢é”™ä¹±
- å®Œç¾ç»“åˆ app5 ç•Œé¢æ•ˆæœå’Œ app6 å¼‚æ­¥èƒ½åŠ›
"""

import argparse
import asyncio
import sys
from collections import deque

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

# æ”¯æŒç›´æ¥è¿è¡Œå’Œæ¨¡å—è¿è¡Œä¸¤ç§æ–¹å¼
if __name__ == "__main__" and __package__ is None:
    import os
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    from apps.app6.config import OPENROUTER_API_KEY, OPENROUTER_BASE_URL, MODEL, TEMPERATURE, MAX_TOKENS
    from apps.app6.tools import get_current_time, end_chat
    from apps.app6.router import route_intent_async, render_history
    from apps.app6.ui import AsyncChatUI, print_user, print_assistant
else:
    from .config import OPENROUTER_API_KEY, OPENROUTER_BASE_URL, MODEL, TEMPERATURE, MAX_TOKENS
    from .tools import get_current_time, end_chat
    from .router import route_intent_async, render_history
    from .ui import AsyncChatUI, print_user, print_assistant


def parse_args():
    parser = argparse.ArgumentParser(description="App6: å¼‚æ­¥èŠå¤©åº”ç”¨")
    parser.add_argument("-i", "--interactive", action="store_true",
                        help="ç®¡é“ç»“æŸåè¿›å…¥äº¤äº’æ¨¡å¼")
    return parser.parse_args()


class AsyncChatSession:
    """å¼‚æ­¥èŠå¤©ä¼šè¯ç®¡ç†å™¨"""

    def __init__(self, llm: ChatOpenAI, ui: AsyncChatUI):
        self.llm = llm
        self.ui = ui
        self.history: list = []
        self.pending_tasks: deque = deque()  # æŒ‰é¡ºåºå­˜å‚¨å¾…å¤„ç†ä»»åŠ¡
        self.should_exit = False

    async def process_input(self, user_input: str, from_pipe: bool = False):
        """å¤„ç†ç”¨æˆ·è¾“å…¥"""
        if from_pipe:
            print_user(user_input)

        self.history.append(HumanMessage(content=user_input))

        # åˆ›å»ºå¤„ç†ä»»åŠ¡
        task = asyncio.create_task(self._handle_response(user_input))
        self.pending_tasks.append(task)
        self.ui.update_pending(1)

    async def _handle_response(self, user_input: str):
        """å¤„ç†å•ä¸ªå“åº”"""
        try:
            action = await route_intent_async(self.llm, user_input, self.history)

            if action == "time":
                result = get_current_time.invoke({})
                reply = f"å½“å‰æ—¶é—´ï¼š{result}"
                self.history.append(AIMessage(content=reply))
                print_assistant(reply, tool_name="get_current_time")

            elif action == "end":
                result = end_chat.invoke({"reason": user_input})
                self.history.append(AIMessage(content=result))
                print_assistant(result, tool_name="end_chat")
                self.should_exit = True

            elif action == "summary":
                sys_msg = SystemMessage(content="ç”¨ 2-3 å¥è¯ç®€æ´æ€»ç»“è¿™æ®µå¯¹è¯çš„ä¸»è¦å†…å®¹ã€‚")
                human_msg = HumanMessage(content=f"å¯¹è¯å†…å®¹ï¼š\n{render_history(self.history)}")
                try:
                    resp = await self.llm.ainvoke([sys_msg, human_msg])
                    reply = resp.content
                except Exception as exc:
                    reply = f"æ€»ç»“å¤±è´¥ï¼š{exc}"
                self.history.append(AIMessage(content=reply))
                print_assistant(reply, tool_name="LLM:summary")

            else:  # chat
                sys_msg = SystemMessage(
                    content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ä¸­æ–‡åŠ©æ‰‹ã€‚æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœç”¨æˆ·é—®æ—¶é—´æ®µï¼ˆå¦‚ä¸Šåˆ/ä¸‹åˆï¼‰ï¼Œè¯·åŸºäºä¹‹å‰è·å–çš„æ—¶é—´ä¿¡æ¯å›ç­”ã€‚"
                )
                try:
                    resp = await self.llm.ainvoke([sys_msg] + self.history)
                    reply = resp.content
                except Exception as exc:
                    reply = f"å›ç­”å¤±è´¥ï¼š{exc}"
                self.history.append(AIMessage(content=reply))
                print_assistant(reply, tool_name="LLM:chat")

        finally:
            self.ui.update_pending(-1)

    async def wait_all_pending(self):
        """ç­‰å¾…æ‰€æœ‰å¾…å¤„ç†ä»»åŠ¡å®Œæˆ"""
        while self.pending_tasks:
            task = self.pending_tasks.popleft()
            await task


async def async_main():
    """å¼‚æ­¥ä¸»å‡½æ•°"""
    args = parse_args()

    llm = ChatOpenAI(
        api_key=OPENROUTER_API_KEY,
        base_url=OPENROUTER_BASE_URL,
        model=MODEL,
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
    )

    # å¤„ç†ç®¡é“è¾“å…¥
    is_piped = not sys.stdin.isatty()

    # åˆå§‹åŒ–å†å²
    history: list = []

    if is_piped:
        piped_lines = [line.strip() for line in sys.stdin.read().splitlines() if line.strip()]

        # ç®¡é“æ¨¡å¼ï¼šä½¿ç”¨ç®€å•è¾“å‡º
        ui = AsyncChatUI()
        session = AsyncChatSession(llm, ui)

        for line in piped_lines:
            await session.process_input(line, from_pipe=True)
            await session.wait_all_pending()
            if session.should_exit:
                return

        # ä¿ç•™ç®¡é“æ¨¡å¼çš„å†å²
        history = session.history

        if not args.interactive:
            return

        # è¿›å…¥äº¤äº’æ¨¡å¼
        from rich.console import Console
        Console().print("\n[dim]â”€â”€â”€ è¿›å…¥äº¤äº’æ¨¡å¼ â”€â”€â”€[/dim]\n")

    # äº¤äº’æ¨¡å¼ï¼šä½¿ç”¨ app5 ç•Œé¢ + å¼‚æ­¥å¤„ç†
    ui = AsyncChatUI()
    pending_tasks = []

    while True:
        # è·å–è¾“å…¥ï¼ˆä¼šå…ˆæ˜¾ç¤ºæ‰€æœ‰å¾…è¾“å‡ºçš„å“åº”ï¼‰
        user_input = await ui.get_input_async()
        if user_input is None:
            break

        # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
        ui.add_pending_output("user", user_input)
        ui.flush_pending_outputs()

        history.append(HumanMessage(content=user_input))

        # åå°å¤„ç†ï¼ˆä¸ç­‰å¾…ï¼‰- ä½¿ç”¨ default å‚æ•°æ•è·å½“å‰è¾“å…¥
        async def process(msg=user_input):
            try:
                action = await route_intent_async(llm, msg, history)

                if action == "time":
                    result = get_current_time.invoke({})
                    reply = f"å½“å‰æ—¶é—´ï¼š{result}"
                    history.append(AIMessage(content=reply))
                    ui.add_pending_output("assistant", reply, tool_name="get_current_time")

                elif action == "end":
                    result = end_chat.invoke({"reason": user_input})
                    history.append(AIMessage(content=result))
                    ui.add_pending_output("assistant", result, tool_name="end_chat")
                    ui.should_exit = True

                elif action == "summary":
                    sys_msg = SystemMessage(content="ç”¨ 2-3 å¥è¯ç®€æ´æ€»ç»“è¿™æ®µå¯¹è¯çš„ä¸»è¦å†…å®¹ã€‚")
                    human_msg = HumanMessage(content=f"å¯¹è¯å†…å®¹ï¼š\n{render_history(history)}")
                    try:
                        resp = await llm.ainvoke([sys_msg, human_msg])
                        reply = resp.content
                    except Exception as exc:
                        reply = f"æ€»ç»“å¤±è´¥ï¼š{exc}"
                    history.append(AIMessage(content=reply))
                    ui.add_pending_output("assistant", reply, tool_name="LLM:summary")

                else:  # chat
                    sys_msg = SystemMessage(
                        content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ä¸­æ–‡åŠ©æ‰‹ã€‚æ ¹æ®å¯¹è¯ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
                    )
                    try:
                        resp = await llm.ainvoke([sys_msg] + history)
                        reply = resp.content
                    except Exception as exc:
                        reply = f"å›ç­”å¤±è´¥ï¼š{exc}"
                    history.append(AIMessage(content=reply))
                    ui.add_pending_output("assistant", reply, tool_name="LLM:chat")

            finally:
                ui.update_pending(-1)

        # å¯åŠ¨åå°ä»»åŠ¡
        task = asyncio.create_task(process())
        pending_tasks.append(task)
        ui.update_pending(1)

        # å¦‚æœç”¨æˆ·è¦é€€å‡ºï¼Œç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if ui.should_exit:
            for task in pending_tasks:
                await task
            break


def main():
    """å…¥å£å‡½æ•°"""
    asyncio.run(async_main())


if __name__ == "__main__":
    main()
